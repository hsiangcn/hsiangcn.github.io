---
layout: post
title:  接口CloseWait持续升高后服务应用Down
categories: 工具
description: 接口CloseWait持续升高后服务应用Down
keywords: java, closeWait
---

这个问题是由于在一次发版后，接口的性能大大降低后对程序进行优化后紧急发版出现的，主要优化的方向是增加异步处理，缓存数据，减少http请求。
服务器的架构，说起服务器的架构就神烦，反正就是一个字：烂。以前也没进行过压测，在生产环境居然还跑的飞起。抱怨两句！！！
懒得画图，反正就是自己记录下，这个接口是给门店使用的，每家门店（或几家门店）都有属于自己的服务器，还是window系统和Linux系统混搭。然后门店服务器请求云端
（说的好听好云端，就是两台服务器搭建后用作中转服务器接口到公司内部），云端访问公司内部接口，说到这不得不吐槽的是，门店系统可以直接ping通公司内部网络，还要云端岂不是浪费。
门店系统和云端有自己的数据库，用来保存一些信息。
门店系统（每台服务器部署了四个节点）-云端系统（两台服务器，13个节点）-公司内部接口-DB,门店系统的应用和云端的还是同一个项目。
回归正题，紧急发版发版后（因为是门店使用，所以也只能在晚上门店关门后才能发版），当时测试没有问题，就放心的去睡觉了，第二天门店一开门就开始上报说接口很慢，超时！通过zabbix监控发现CloseWait一直持续升高，
最后应用服务直接挂掉在没有请求进来。就只能紧急回滚版本！中间持续了大概1个小时，然后把日志拉下来分析发现，接口日志没有任何异常，日志打印的接口耗时也都正常，平均耗时大概是：500ms，这个时间也是能接受的。
然后找DBA帮查了下有没有异常的SQL，在云端数据库发现有大量的锁等待，系统是没有任何事务的。这个就肯定是并发导致的，在程序中加分布式锁，数据库中加索引，优化后
我们就在本地反复压测，所有的接口（三个接口）TPS在100-150之间，日志也没有任何异常，应用也没有问题！反复压测了几天没问题，我们就发版了，这次有了很大的提升，在早上和中午高峰的时候
CloseWait就升高，这次是过个几分钟就会降下来，想比上次升高就直接挂点有了很明显的提升，整天我们就在观察zabbix监控、分析日志和sql！一直到下午的时候还是没有找到原因迫于无奈又一次回滚了。
相比上次这次有了充足的日志和数据可以用来进行分析，花了一周的时间将所有的日志，sql分析了一遍，整个代码又复习了一遍，反复的压测一直没有找到问题，也没有复现！但是真快到奔溃边缘了，一个问题找了一周，没有任何进展。
